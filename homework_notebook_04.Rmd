---
title: "automatization_notebook_04"
output: word_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(ggplot2)
library(psych)
library(readr)
library(mice)
library(VIM)
library(tableone)
library(ggbeeswarm)
library(RColorBrewer)
library(patchwork)
library(gridExtra)
library(car)
library(corrplot)
library(ROCR)

```

# Чтение данных

В вашем варианте нужно использовать датасет healthcare-dataset-stroke-data.

```{r}
data <- read_csv("data/raw/healthcare-dataset-stroke-data.csv")

glimpse(data)

```

# Выведите общее описание данных

```{r}

str(data)
summary(data)


```

# Очистка данных

1) Уберите переменные, в которых пропущенных значений больше 20% или уберите субъектов со слишком большим количеством пропущенных значений. Или совместите оба варианта. Напишите обоснование, почему вы выбрали тот или иной вариант:

2) Переименуйте переменные в человекочитаемый вид (что делать с пробелами в названиях?);

3) В соответствии с описанием данных приведите переменные к нужному типу (numeric или factor);

4) Отсортируйте данные по возрасту по убыванию;

5) Сохраните в файл outliers.csv субъектов, которые являются выбросами (например, по правилу трёх сигм) — это необязательное задание со звёздочкой;

6) Присвойте получившийся датасет переменной "cleaned_data".

```{r}
#1)Проверка на пропущенные значения
sum(is.na(data))
aggr(data)

data$gender %>% unique()
data$ever_married %>% unique()
data$work_type %>% unique()
data$Residence_type %>% unique()
data$bmi %>% unique()
data$smoking_status %>% unique()
data$stroke %>% unique()

data %>% nrow() -> data_na
data %>% filter(bmi == "N/A") %>% nrow() -> bmi_na
data %>% filter(smoking_status == "Unknown") %>% nrow() -> smoke_na

na_lev_bmi <- (bmi_na/data_na)*100
na_lev_smoke <- (smoke_na/data_na)*100

print("Обоснование: пропущенные значения встречаются в переменной bmi, в количестве 201 из 5110 (3.93%), данные из этой переменной не попадают под критерии удаления из анализа. Также в переменной smoking_status большое количество испытуемых имеют значение unknown (30.22 %), поэтому считаю целесообразным исключить данную переменную из анализа.")

data$bmi[data$bmi == "N/A"] <- NA

#2-4, 6)

data %>% mutate(bmi = as.numeric(bmi)) %>% rename(residence_type = Residence_type, glucose_level = avg_glucose_level) %>% select(!smoking_status) %>% mutate(gender = as.factor(gender), 
                                                          hypertension =  as.factor(hypertension),
                                                          heart_disease = as.factor(heart_disease),
                                                          ever_married = as.factor(ever_married),
                                                          work_type = as.factor(work_type),
                                                          residence_type = as.factor(residence_type),
                                                          stroke = as.factor(stroke)) %>% 
    arrange(desc(age)) -> cleaned_data

#5) В датасете 3 числовых переменных, по которым можно выделить выбросы: возраст, уровень глюкозы и ИМТ:

cleaned_data %>% filter(abs(age - mean(age)) > 3*sd(age) | abs(glucose_level - mean(glucose_level)) > 3*sd(glucose_level) | abs(bmi - mean(bmi)) > 3*sd(bmi)) -> outliers

write_csv(outliers, "data/raw/outliers.csv")
  
```

# Сколько осталось переменных?

```{r}
cleaned_data %>% ncol() %>% print()


```

# Сколько осталось случаев?

```{r}
cleaned_data %>% nrow()


```

# Есть ли в данных идентичные строки?

```{r}

cleaned_data %>% duplicated() %>% sum() -> a
if (a > 0) {
  print("да, ", a)
} else print("нет")

```

# Сколько всего переменных с пропущенными значениями в данных и сколько пропущенных точек в каждой такой переменной?

```{r}

cleaned_data %>% select_if(~any(is.na(.))) %>% filter(is.na(.)) %>% nrow()

#201 пропущенное значение в переменной bmi

```

# Описательные статистики

## Количественные переменные

1) Рассчитайте для всех количественных переменных для каждой группы (stroke):

1.1) Количество значений;

1.2) Количество пропущенных значений;

1.3) Среднее;

1.4) Медиану;

1.5) Стандартное отклонение;

1.6) 25% квантиль и 75% квантиль;

1.7) Интерквартильный размах;

1.8) Минимум;

1.9) Максимум;

1.10) 95% ДИ для среднего - задание со звёздочкой.

```{r}

cleaned_data %>% select(stroke, where(is.numeric)) %>% group_by(stroke) %>% summarise(
    сount = n(),                  # Количество значений
    missing_values = sum(is.na(age) | is.na(bmi) | is.na(glucose_level)),  # Количество пропущенных значений
    mean_age = mean(age, na.rm = TRUE),     # Среднее age
    median_age = median(age, na.rm = TRUE), # Медиана age
    SD_age = sd(age, na.rm = TRUE),         # Стандартное отклонение age
    Q1_age = quantile(age, 0.25, na.rm = TRUE),    # 25% квантиль age
    Q3_age = quantile(age, 0.75, na.rm = TRUE),    # 75% квантиль age
    IQR_age = IQR(age, na.rm = TRUE),             # Интерквартильный размах age
    min_age = min(age, na.rm = TRUE),             # Минимум age
    max_age = max(age, na.rm = TRUE),             # Максимум age
    CI_mean_age_l = mean_age - (qnorm(0.975) * (SD_age / sqrt(n()))),  # 95% ДИ для среднего age, нижняя граница
    CI_mean_age_up = mean_age + (qnorm(0.975) * (SD_age / sqrt(n()))),  # 95% ДИ для среднего age, верхняя граница
    mean_BMI = mean(bmi, na.rm = TRUE),     # Среднее bmi
    median_BMI = median(bmi, na.rm = TRUE), # Медиана bmi
    SD_BMI = sd(bmi, na.rm = TRUE),         # Стандартное отклонение bmi
    Q1_BMI = quantile(bmi, 0.25, na.rm = TRUE), # 25% квантиль bmi
    Q3_BMI = quantile(bmi, 0.75, na.rm = TRUE), # 75% квантиль bmi
    IQR_BMI = IQR(bmi, na.rm = TRUE), # Интерквартильный размах bmi
    min_BMI = min(bmi, na.rm = TRUE), # Минимум bmi
    max_BMI = max(bmi, na.rm = TRUE), # Максимум bmi
    CI_mean_BMI_l = mean_BMI - (qnorm(0.975) * (SD_BMI / sqrt(n()))),  # 95% ДИ для среднего BMI, нижняя граница
    CI_mean_BMI_up = mean_BMI + (qnorm(0.975) * (SD_BMI / sqrt(n()))),  # 95% ДИ для среднего BMI, верхняя граница
    mean_glucose = mean(glucose_level, na.rm = TRUE),     # Среднее glucose_level
    median_glucose = median(glucose_level, na.rm = TRUE), # Медиана glucose_level
    SD_glucose = sd(glucose_level, na.rm = TRUE),   # Стандартное отклонение glucose_level
    Q1_glucose = quantile(glucose_level, 0.25, na.rm = TRUE), # 25% квантиль glucose_level
    Q3_glucose = quantile(glucose_level, 0.75, na.rm = TRUE), # 25% квантиль glucose_level
    IQR_glucose = IQR(glucose_level, na.rm = TRUE), # Интерквартильный размах glucose_level
    min_glucose = min(glucose_level, na.rm = TRUE), # Минимум glucose_level
    max_glucose = max(glucose_level, na.rm = TRUE), # Максимум glucose_level
    CI_mean_glucose_l = mean_glucose - (qnorm(0.975) * (SD_glucose / sqrt(n()))),  # 95% ДИ для среднего glucose_level, нижняя граница
    CI_mean_glucose_up = mean_glucose + (qnorm(0.975) * (SD_glucose / sqrt(n())))  # 95% ДИ для среднего glucose_level, верхняя граница
  ) -> stat_data

stat_data %>% filter(stroke == 0) %>% pivot_longer(cols = -stroke, names_to = "parameters_stroke_0", values_to = "values_stroke_0") %>% select(!stroke) %>% mutate(values_stroke_0 = round(values_stroke_0, 2)) -> stroke_0

stat_data %>% filter(stroke == 1) %>% pivot_longer(cols = -stroke, names_to = "parameters_stroke_1", values_to = "values_stroke_1") %>% select(!stroke) %>% mutate(values_stroke_1 = round(values_stroke_1, 2)) -> stroke_1

print(stroke_0)
print(stroke_1)

```

## Категориальные переменные

1) Рассчитайте для всех категориальных переменных для каждой группы (stroke):

1.1) Абсолютное количество;

1.2) Относительное количество внутри группы;

1.3) 95% ДИ для доли внутри группы - задание со звёздочкой.

```{r}

cleaned_data %>% select(where(is.factor) & !stroke) %>% colnames() -> factor_vars

#Таблица частот

table_prop <- CreateTableOne(data = cleaned_data, vars = factor_vars, strata = "stroke")
prop_v <- print(table_prop, exact = "all", test = FALSE, cmd = TRUE, ci = TRUE)

#Рассчет ДИ для доли внутри группы

prop_v_num <- as.data.frame(gsub("\\(.*?\\)", "", prop_v))
 
prop_v_num <- as.data.frame(lapply(prop_v_num, as.numeric))

row_names <- c("all", "gender_female", "gender_male", "gender_other", "hypertension_1", "heart_disease_1", "ever_married_Yes", "work_type_children", "work_type_Govt_job", "work_type_Never_worked", "work_type_Private", "work_type_Self-employed", "residence_type_Urban")

prop_v_num %>% filter(!is.na(prop_v_num$X0)) -> prop_v_num

c_i <- function(success, total) {
  pt_result <- prop.test(success, total)
  ci <- pt_result$conf.int
  return(ci)
}


prop_v_num <- prop_v_num %>%
  mutate(total_0 = prop_v_num[1, 1], total_1 = prop_v_num[1, 2]) %>%
  rowwise() %>%
  mutate(
    ci_0 = list(c_i(X0, total_0)),
    ci_1 = list(c_i(X1, total_1))
  )

rownames(prop_v_num) <- row_names

prop_v_num %>% select(ci_0, ci_1) -> ci_stroke
print(ci_stroke)


```

# Визуализация

## Количественные переменные

1) Для каждой количественной переменной сделайте боксплоты по группам. Расположите их либо на отдельных рисунках, либо на одном, но читаемо;

2) Наложите на боксплоты beeplots - задание со звёздочкой.

3) Раскрасьте боксплоты с помощью библиотеки RColorBrewer.

```{r}
colors <- c("blue3", "chartreuse4")

boxplots <- lapply(c("age", "bmi", "glucose_level"), function(var) {
  p <- ggplot(cleaned_data, aes(x = stroke, y = !!sym(var), fill = stroke)) +
    geom_boxplot(position = position_dodge2(preserve = "single"), fill = colors) +
    geom_beeswarm(position = position_dodge2(width = 0.01), alpha = 0.1, size = 0.1, color = "blue1") +
    scale_fill_brewer(palette = "Set1") +
    labs(title = var, x = "Stroke", y = var) +
    theme_minimal()
  return(p)
})

boxplots_combined <- wrap_plots(boxplots, ncol = 2) 

print(boxplots_combined)

```

## Категориальные переменные

1) Сделайте подходящие визуализации категориальных переменных. Обоснуйте, почему выбрали именно этот тип.

```{r}

# Цель визуализации категориальных переменных - показать наличие или отсутствие различий в количестве значений представленных категорий для каждой переменной. Подходящим графиком является столбчатая диаграмма. В зависимости от задачи, например, при необходимости сравнения количественных показателей в зависимости от принадлежности к различным категориям, может использоваться box plot, violin plot, swarm plot или point plot. Но, если я корректно понимаю задание, в данном случае такой задачи нет.

cleaned_data %>% select(where(is.factor)) -> factor_vars

plots <- function(data, var_name) {
  pl <- ggplot(data, aes(x = data[[var_name]])) +
    geom_bar(fill = "#330099") +
    geom_text(stat = "count", aes(label = after_stat(count)), vjust = 0, size = 2) +  
    labs(title = var_name, x = NULL) + 
     theme(axis.text.x = element_text(size = 5), 
          axis.text.y = element_text(size = 5),  
          axis.title = element_text(size = 5))
  return(pl)
}

barplots <- lapply(names(factor_vars), function(var) {
  plots(cleaned_data, var)
})

barplots_combined <- wrap_plots(barplots, ncol = 3) 

print(barplots_combined)
```


# Статистические оценки

## Проверка на нормальность

1) Оцените каждую переменную на соответствие нормальному распределению с помощью теста Шапиро-Уилка. Какие из переменных являются нормальными и как как вы это поняли?

```{r}
# Так как в нашей выборке больше 5000 наблюдений, мы не можем воспользоваться тестом Шапиро-Уилка для проверки распределения на нормальность в общей выборке, но можем проверить распределения на нормальность внутри групп по переменной stroke.

stroke_0_data <- cleaned_data %>%
  filter(stroke == 0)

stroke_1_data <- cleaned_data %>%
  filter(stroke == 1)

vars_0 <- stroke_0_data %>% select(age, bmi, glucose_level)

vars_1 <- stroke_1_data %>% select(age, bmi, glucose_level)

sh_w_0 <- lapply(vars_0, function(x) {
 result <- shapiro.test(x)
  return(data.frame(
    variable = deparse(substitute(x)),
    W_statistic = result$statistic,
    p_value = result$p.value
  ))
})
sh_w_1 <- lapply(vars_1, function(x) {
  result <- shapiro.test(x)
  return(data.frame(
    variable = deparse(substitute(x)),
    W_statistic = result$statistic,
    p_value = result$p.value
  ))
})

sh_w_results <- bind_rows(
  data.frame(Group = "Stroke 0", sh_w_0),
  data.frame(Group = "Stroke 1", sh_w_1)
)

sh_w_results %>% select(-contains(".variable")) %>% 
                pivot_longer(
               cols = -Group,
               names_to = "parameter",
               values_to = "value") %>% print()

# В полученных результатах теста Шапиро-Уилка для всех количественных переменных  в обеих группах вероятности получения рассчитанных W-статистик при нормальносм распределении меньше 0.05, таким образом, мы можем отклонить нулевую гипотезу о нормальности распределения значений возраста, среднего уровня глюкозы крови и ИМТ в группах субъектов исследования с инсультом и без инсульта.

```

2) Постройте для каждой количественной переменной QQ-плот. Отличаются ли выводы от теста Шапиро-Уилка? Какой метод вы бы предпочли и почему?

```{r}

# Так как для построения графика нет ограничений на объем выборки, мы можем оценить распределения для общей выборки:

cleaned_data %>% select(age, bmi, glucose_level) -> num_vars

qqpl <- function(data, var_name) {
  ggplot(data, aes(sample = .data[[var_name]])) +
    stat_qq() +
    geom_qq_line(color = "red") +
    labs(title = paste("Q-Q Plot for", var_name), labels = "AUTO") 
    
}

# Создаем Q-Q графики для каждой переменной
qqplots <- lapply(names(num_vars), function(var) {
  qqpl(cleaned_data, var)
})

qqplots_combined <- wrap_plots(qqplots, ncol = 2) 

print(qqplots_combined)

# Распределения оцененных количественных переменных отличаются от нормального. Для переменной возраста значения в переделах (-4, -2) и (2,4) нормальных квантилей выше и ниже значений, ожидаемых при нормальном распределении, соответственно (на гистограмме распрееления мы увидим смещение вправо, скошенность графика влево). При этом slope диагональной линии на графике остается около 45 градусов, но линия смещена вправо. 

# Для переменной ИМТ (bmi) график гистограммы будет более широкий с обеих сторон, чем при нормальном распределении. slope меньше 45 градусов.
# Для среднего уровня глюкозы гистгграмма распределения будет также отличаться от нормального распределения, как и для ИМТ, но с более высокими значениями для 1-4 нормальных квантилей. slope меньше 45 градусов.


# Для сравнения интерпретации с методом Шапиро-Уилка из предыдущего задания построим графики распределения значения количественных переменных для групп субьектов исследования с различным статусом по наличию инсульта:

stroke_0_data <- cleaned_data %>%
  filter(stroke == 0)

stroke_1_data <- cleaned_data %>%
  filter(stroke == 1)

vars_0 <- stroke_0_data %>% select(age, bmi, glucose_level)

vars_1 <- stroke_1_data %>% select(age, bmi, glucose_level)

qqpl_01 <- function(data, var_name, group) {
  ggplot(data, aes(sample = .data[[var_name]])) +
    stat_qq() +
    geom_qq_line(color = "red") +
    labs(title = paste("Q-Q Plot for", var_name), subtitle = group)
}

qqplots_0 <- lapply(names(vars_0), function(var) {
  qqpl_01(cleaned_data, var, "Stroke = 0")
})
qqplots_1 <- lapply(names(vars_1), function(var) {
  qqpl_01(cleaned_data, var, "Stroke = 1")
})

combined_qq_plots <- grid.arrange(grobs = c(qqplots_0, qqplots_1), ncol = 3) 

print(combined_qq_plots)

# Мы видим, что графики для групп с разным статусом инсульта сходны с результатами для общей выборки. Выводы о норамальности распределения, которые можно сделать при анализе полученных графиков соответствуют выводам по результатам теста Шапиро-Уилка, при этом при интерпретации графиков мы получаем информацию не только о том, отличается ли распределение от нормального, но и о том, как оно отличечается от нормального. Считаю, что оценка нормальности распределения методом построения графиков QQ-плот дает больше информации о характере распределения.


```

3) Ниже напишите, какие ещё методы проверки на нормальность вы знаете и какие у них есть ограничения.

Еще один метод визуализации, который дает представление о характере распределения значений признака - построение гистограммы. Можно качественно оценить сходство распределения с колокообразной формой нормального распределения. Но внешний вид гистограммы сильно зависит от количества наблюдений и выбранного шага, при малых выборках будет сложно оценить вид распределения. 

Также есть ряд других статистических критериев проверки распределения на нормальность. В ГОСТ Р ИСО 5479-2002 “Статистические методы. Проверка отклонения распределения вероятностей от нормального распределения” описаны следующие методы: 
- критерий проверки на симметричность (нулевая гипотеза - распределение симметрично, альтернатива - положительная/отрицательная асимметрия): неотклонение симметричности не подтверждает нормальности распределения (является необходимым, но не достаточным условием), применяется совместно с критерием проверки на эксцесс
- критерий проверки на эксцесс: объем выборки от 8 до 5000 наблюдений, мощность сильно зависит от объема выборки.
- критерий Эппса-Палли: при небольших объемах выборок высокий процент ошибок второго рода.
По рейтингу мощности критерий Шапиро-Уилка является наиболее оптимальным, но ограничен в применении при альтернативной гипотизе с симметричным распределением (в таком случае рекомендуется применение критерия Эппса-Палли), малых обьемах выборок и обьемах выборок более 5000.


## Сравнение групп

1) Сравните группы (переменная **stroke**) по каждой переменной (как количественной, так и категориальной). Для каждой переменной выберите нужный критерий и кратко обоснуйте его выбор в комментариях.

```{r}
# Для выбора подходящего критерия для количественных переменных предварительно можно оценить наличие различий в десперсиях между группами (тест Левена, так как распределение отличается от нормального): 
cleaned_data %>% select(stroke, age, bmi, glucose_level) -> num_vars

num_vars %>%
  summarise(
    age_levene = leveneTest(age ~ stroke, data = .)$Pr,
    bmi_levene = leveneTest(bmi ~ stroke, data = .)$Pr,
    glucose_level_levene = leveneTest(glucose_level ~ stroke, data = .)$Pr
  ) -> lev_res

print(lev_res)

# Для переменных age, bmi и glucose_level выявлены различия в дисперсиях между группами по stroke, верояность получить такие значения при условии, что верна нулевая гипотеза о гомогенности дисперсий, менее 0.05

# Для количественных переменных, учитывая, что распределение значений отличается от нормального, а также различия в дисперсиях, для обнаружения различий средних значений у субьектов со значением stroke 0 и 1, следует применить критерий Манна-Уитни.


cleaned_data %>% 
  summarise(age_mw = wilcox.test(age ~ stroke)$p.value,
    bmi_mw = wilcox.test(bmi ~ stroke )$p.value,
    glucose_level_mw = wilcox.test(glucose_level ~ stroke)$p.value) %>% print() 

# Для категориальных переменных при выборе критерия следует учитывыть количество наблюдений для каждой группы, а также количество значений, которые может принимать категориальная переменная. Для переменных, принимающих 2 значения, используя данные о количестве значений, полученный в предыдущих заданиях, можем считать подходящим критери хи-квадрат (переменные heart_disease, ever_married, residence_type, hypertension). Для переменной gender и work_type в некоторых градациях количество значений меньше 5, следовательно, корректно прменение критерия Фишера. Для переменной work_type из-за большого числа градаций целесообразно прмененеие метода Монте-Карло для оценки значимости. 

cleaned_data %>% select(where(is.factor) & !stroke &!gender & !work_type) -> factor_vars

lapply(names(factor_vars), function(var) {
    print(var)
    cross_tab <- table(cleaned_data[[var]], cleaned_data$stroke) 
    print(cross_tab)
    chi_square <- chisq.test(cross_tab)  
    print(chi_square)
  })

var <- cleaned_data %>% select(gender)
lapply(names(var), function(var) {
  print(var) 
  cross_tab <- table(cleaned_data[[var]], cleaned_data$stroke) 
  print(cross_tab) 
  fisher_test <- fisher.test(cross_tab) 
  print(fisher_test)
})

var <- cleaned_data %>% select(work_type)
lapply(names(var), function(var) {
  print(var) 
  cross_tab <- table(cleaned_data[[var]], cleaned_data$stroke) 
  print(cross_tab) 
  fisher_test <- fisher.test(cross_tab, simulate.p.value=TRUE) 
  print(fisher_test)
})
```

# Далее идут **необязательные** дополнительные задания, которые могут принести вам дополнительные баллы в том числе в случае ошибок в предыдущих

## Корреляционный анализ

1) Создайте корреляционную матрицу с визуализацией и поправкой на множественные сравнения. Объясните, когда лучше использовать корреляционные матрицы и в чём минусы и плюсы корреляционных исследований.

```{r}
# Рассчет коэффициента корреляции использует средние значения, поэтому чувствителен к выбросам и при распределении, отличном от нормального. Также рассчет коэффициента корреляции не позволяет выявлять нелинейные взаимосвязи между значениями переменных. Так как распределения количественных переменных в анализируемых данных отличаются от нормального, используем коэффициент корреляции Спирмана.

cleaned_data %>% select(age, bmi, glucose_level) -> cor_data 
cor_data %>% corr.test(method = "spearman")  -> cor_test
print(cor_test)

scatterplots <- lapply(c("age", "bmi", "glucose_level"), function(var1) {
  lapply(c("age", "bmi", "glucose_level"), function(var2) {
    scat <- ggplot(cor_data, aes_string(x = var1, y = var2)) +
      geom_point(size = 0.2) +
      labs(title = paste(var1, var2, sep = " vs. "), x = var1, y = var2) +
      theme_minimal()
    return(scat)
  })
})

scatter_plots <- do.call(c, scatterplots)
grid.arrange(grobs = scatter_plots, ncol = 3)

corrplot(corr = cor_test$r, p.mat = cor_test$p, method = "color", order = "hclust")


```

## Моделирование

1) Постройте регрессионную модель для переменной **stroke**. Опишите процесс построения. 

Так как stroke - категориальная переменная, строим логистическую регрессию. Сначала строим для всех переменных, после оценки результата строим модели с другими комбинациями, выбираем оптимальную. В переменной bmi есть пропущенные значения, которые удалить/заполнить средними значениями. Так как размер выборки достаточно большой, удалим значения.

```{r}
data1 <- cleaned_data %>% filter(!is.na(bmi)) %>% select(!id)
model_1 <-  glm(stroke ~ ., data = data1, family = binomial())
summary(model_1)

# показатель AIC = 1396.5 (чем он меньше, тем лучше модель). Уменьшим количество параметров, оставим те, для которых получили показатели p <0.05 (значимое влияние на значение переменной stroke)

model_2 <-  glm(stroke ~ glucose_level + hypertension + age, data = data1, family = binomial())
summary(model_2)

# показатель AIC = 1386.4, практически не изменился.
# применим функцию step() для автоматического подбора моделей.

step(model_1, direction = "backward")

# согласно полученным результатам, наименьший AIC - для следующей модели:

model_final <- glm(stroke ~ age + hypertension + heart_disease + glucose_level, data = data1, family = binomial())
summary(model_final)

# Согласно построенной модели, все включенные в построение переменные влияют на вероятность инсульта (возраст, наличие АГ, болезней сердца, средний уровень глюкозы крови).

#Воспользуемся данной моделью для рассчета вероятности развития инсульта для пациентов данной выборки.

data1$prob <- predict(object = model_final, type = "response") %>% as.numeric()

data1

# построим ROC-кривую для определения оптимального значения соотношения ложно-положительных и ложноотрицательных результатов, при котором мы с наибольшей точностью можем предсказать развитие инсульта у пациента.
pred <- prediction(data1$prob, data1$stroke)
perf <- performance(pred, "tpr", "fpr")
plot(perf, colorsize = TRUE)

auc <- performance(pred, measure = "auc")
str(auc)

#Площадь под кривой = 0.85, найдем порог отсечения

perf1 <- performance(pred, x.measure = "cutoff", measure = "spec")
perf2 <- performance(pred, x.measure = "cutoff", measure = "sens")
perf3 <- performance(pred, x.measure = "cutoff", measure = "acc")

plot(perf1, col = "red", lwd =2)
plot(add = TRUE, perf2, col = "blue", lwd =2)
plot(add = TRUE, perf3, col = "green", lwd =2)

# точка пересечения для чувствительности, специфичности и точности находится на значении Cutoff приблизительно 0.055, добавим в исходную таблицу предсказанные значения:

data1$pr_stroke <- factor(ifelse(data1$prob > 0.055, 1, 0))

data1$correct <- factor(ifelse(data1$pr_stroke == data1$stroke, "Y", "N"))

data1

ggplot(data1, aes(prob, fill = factor(correct)))+
  geom_dotplot()+
  theme(axis.text=element_text(size=25),
        axis.title=element_text(size=25,face="bold"))

# мы получили очень высокий процент ложноположительных результатов и низкий процент ложноотрицательных, что в случае построения модели с целю профилактики инсульта может быть целесообразным. Но высокий AIC и высокий процент ложноположительных результатов говорит о, возможно, низком качестве модели.
```




